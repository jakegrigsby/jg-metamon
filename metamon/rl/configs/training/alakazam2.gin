import amago.agent

agent.Agent.reward_multiplier = 10.
agent.MultiTaskAgent.reward_multiplier = 10.

agent.Agent.tau = .008
agent.MultiTaskAgent.tau = .008

agent.Agent.num_actions_for_value_in_critic_loss = 1
agent.MultiTaskAgent.num_actions_for_value_in_critic_loss = 4

agent.Agent.num_actions_for_value_in_actor_loss = 1
agent.MultiTaskAgent.num_actions_for_value_in_actor_loss = 4

agent.Agent.online_coeff = 0.1
agent.MultiTaskAgent.online_coeff = 0.1
agent.Agent.offline_coeff = 1.0
agent.MultiTaskAgent.offline_coeff = 1.0
agent.Agent.fbc_filter_func = @agent.leaky_relu_filter
agent.MultiTaskAgent.fbc_filter_func = @agent.leaky_relu_filter
agent.leaky_relu_filter.beta = .4
agent.leaky_relu_filter.tau = 1e-3
agent.leaky_relu_filter.neg_slope = .05
agent.leaky_relu_filter.clip_weights_low = 1e-3
agent.leaky_relu_filter.clip_weights_high = 15.

MetamonAMAGOExperiment.l2_coeff = 1e-4
MetamonAMAGOExperiment.learning_rate = 1.25e-4
MetamonAMAGOExperiment.grad_clip = 1.5
MetamonAMAGOExperiment.critic_loss_weight = 13.5
MetamonAMAGOExperiment.lr_warmup_steps = 1500
